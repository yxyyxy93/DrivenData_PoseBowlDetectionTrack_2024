{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This is a notebook that will help to get started with the [Pose bowl: detection track competition participation](https://www.drivendata.org/competitions/260/spacecraft-detection/page/833/).\n",
    "\n",
    "> In this challenge, you will help to develop new methods for conducting spacecraft inspections by identifying the boundaries of a target spacecraft in an image.The challenge consists of a spacecraft detection track and a pose estimation track. In the spacecraft detection track (that's this one!) your task will be to draw bounding boxes around the spacecraft in an image.\n",
    "\n",
    "### Objective\n",
    "This notebook is focused on getting started to use ultralytics yolo and fine tune it on this custom spaceship object detection task. This will only help you to get started and get scores better than benchmark but not win!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-19T14:12:42.871894Z",
     "iopub.status.busy": "2024-03-19T14:12:42.870882Z",
     "iopub.status.idle": "2024-03-19T14:13:18.824215Z",
     "shell.execute_reply": "2024-03-19T14:13:18.822601Z",
     "shell.execute_reply.started": "2024-03-19T14:12:42.871836Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "%pip install ultralytics\n",
    "%pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verify installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset preparation\n",
    "Below we will copy the dataset definition yaml file and modify it to be ready to use for trainign wil ultralytics yolo package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset directory\n",
    "Ultralytics Yolo expects the dataset yaml file to be present on *workspace/dataset* directory.\n",
    "In our case we will create */kaggle/working/dataset* directory and copy the spaceship.yaml file to this location. For more information  please visit the [ultralytics documentation](https://docs.ultralytics.com/datasets/detect/#supported-dataset-formats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current folder name is: spacecraft-pose-object-detection-runtime\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "# Get the current working directory's full path\n",
    "current_working_directory = os.getcwd()\n",
    "# Extract the folder name from the full path\n",
    "current_folder_name = os.path.basename(current_working_directory)\n",
    "# Print the current folder name\n",
    "print(\"The current folder name is:\", current_folder_name)\n",
    "\n",
    "# # Define the source directory containing the .tar files and the target directory for extracted files\n",
    "# source_dir = 'Dataset_spacecraft_pose_object_detection'\n",
    "# target_dir = 'data_dev'\n",
    "# # Create the target directory if it doesn't exist\n",
    "# os.makedirs(target_dir, exist_ok=True)\n",
    "# # Walk through the source directory\n",
    "# for root, dirs, files in os.walk(source_dir):\n",
    "#     for file in files:\n",
    "#         # Check if the file is a .tar file\n",
    "#         if file.endswith('.tar'):\n",
    "#             # Construct the full path to the .tar file\n",
    "#             tar_path = os.path.join(root, file)\n",
    "#             print(f'Extracting {tar_path} to {target_dir}')\n",
    "#             # Open the .tar file\n",
    "#             with tarfile.open(tar_path) as tar:\n",
    "#                 # Extract its contents into the target directory\n",
    "#                 tar.extractall(path=target_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred 355/355 items from pretrained weights\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('yolov8n.yaml')  # build a new model from YAML\n",
    "model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n",
    "model = YOLO('yolov8n.yaml').load('yolov8n.pt')  # build from YAML and transfer weights\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the datasets\n",
    "train_labels = pd.read_csv('data_dev/train_labels.csv')\n",
    "train_meta = pd.read_csv('data_dev/train_metadata.csv')\n",
    "\n",
    "# Merge the datasets on image_id\n",
    "merged_data = pd.merge(train_labels, train_meta, on='image_id')\n",
    "image_width, image_height = 1280, 1024\n",
    "\n",
    "# Function to convert the bounding box to the YOLO format\n",
    "def convert_to_yolo_format(bbox, image_width, image_height):\n",
    "    x_center = ((bbox['xmin'] + bbox['xmax']) / 2) / image_width\n",
    "    y_center = ((bbox['ymin'] + bbox['ymax']) / 2) / image_height\n",
    "    width = (bbox['xmax'] - bbox['xmin']) / image_width\n",
    "    height = (bbox['ymax'] - bbox['ymin']) / image_height\n",
    "    # return (bbox['spacecraft_id'], x_center, y_center, width, height)\n",
    "    return (0, x_center, y_center, width, height)\n",
    "    \n",
    "# Directory where you want to save the YOLO formatted annotation files\n",
    "annotations_dir = Path('data_dev/annotations_dir')\n",
    "annotations_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize a counter\n",
    "processed_images = 0\n",
    "\n",
    "# Convert and save the annotations in YOLO format for the first 256 images\n",
    "for index, row in merged_data.iterrows():\n",
    "    # # Break the loop if 256 images have been processed\n",
    "    # if processed_images >= 256:\n",
    "    #     break\n",
    "    \n",
    "    yolo_bbox = convert_to_yolo_format(row, image_width, image_height)\n",
    "    annotation = f'{yolo_bbox[0]} {yolo_bbox[1]:.6f} {yolo_bbox[2]:.6f} {yolo_bbox[3]:.6f} {yolo_bbox[4]:.6f}\\n'\n",
    "    \n",
    "    # Write to a new text file for each image_id\n",
    "    with (annotations_dir / f'{row[\"image_id\"]}.txt').open(mode='w') as file:\n",
    "        file.write(annotation)\n",
    "    \n",
    "    # Increment the counter\n",
    "    processed_images += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify to add correct path\n",
    "We will perform the following\n",
    "* Change the base path to read the dataset directly from */kaggle/input/posebowl/spaceship*\n",
    "* Add test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (5.4.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/xiaoyu/.local/lib/python3.10/site-packages (4.66.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyyaml\n",
    "!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "# Define directories\n",
    "base_dir = Path('data_dev')\n",
    "train_dir = base_dir / 'train'\n",
    "val_dir = base_dir / 'val'\n",
    "annotations_dir = base_dir / 'annotations_dir'\n",
    "\n",
    "# Function to move label files according to the images in a directory\n",
    "def move_corresponding_labels(image_dir, label_dir):\n",
    "    # Ensure the label directory exists\n",
    "    label_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Get a list of all image files\n",
    "    image_files = list(image_dir.glob('*.png'))\n",
    "\n",
    "    # Iterate over each image in the image directory with a progress bar\n",
    "    for image_path in tqdm(image_files, desc=f\"Processing images in {image_dir.name}\"):\n",
    "        # Find the corresponding annotation file\n",
    "        annotation_path = annotations_dir / f\"{image_path.stem}.txt\"\n",
    "        \n",
    "        # Check if the annotation file exists\n",
    "        if annotation_path.exists():\n",
    "            # Move the annotation file to the label directory\n",
    "            shutil.move(str(annotation_path), str(label_dir / annotation_path.name))\n",
    "        else:\n",
    "            print(f\"No label found for image: {image_path.name}\")\n",
    "\n",
    "# Execute the function for both training and validation sets\n",
    "move_corresponding_labels(train_dir / 'images', train_dir / 'labels')\n",
    "move_corresponding_labels(val_dir / 'images', val_dir / 'labels')\n",
    "\n",
    "print(\"Label files have been moved to match the image directories.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm  # Import the tqdm library for the progress bar\n",
    "\n",
    "# Define paths\n",
    "base_dir = Path('data_dev')\n",
    "image_dir = base_dir / 'images'\n",
    "annotations_dir = base_dir / 'annotations_dir'\n",
    "train_dir = base_dir / 'train'\n",
    "val_dir = base_dir / 'val'\n",
    "\n",
    "# Create train and val directories if they don't exist\n",
    "train_dir.mkdir(parents=True, exist_ok=True)\n",
    "val_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Retrieve all image files (assuming .png format)\n",
    "image_files = list(image_dir.glob('*.png'))\n",
    "random.shuffle(image_files)  # Shuffle for random distribution\n",
    "\n",
    "# Define the split ratio for training and validation sets (e.g., 80% training, 20% validation)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(image_files) * split_ratio)\n",
    "\n",
    "# Split the dataset\n",
    "train_files = image_files[:split_index]\n",
    "val_files = image_files[split_index:]\n",
    "\n",
    "# Function to copy image and corresponding annotation files\n",
    "def copy_files(file_list, dest_dir):\n",
    "    # Ensure directories for images and labels exist in train and val folders\n",
    "    (dest_dir / 'images').mkdir(exist_ok=True)\n",
    "    (dest_dir / 'labels').mkdir(exist_ok=True)\n",
    "    \n",
    "    # Copy files with progress bar\n",
    "    for img_path in tqdm(file_list, desc=f\"Copying files to {dest_dir.name}\"):\n",
    "        # Copy image\n",
    "        shutil.copy(img_path, dest_dir / 'images' / img_path.name)\n",
    "        # Copy corresponding annotation file, if exists\n",
    "        annotation_path = annotations_dir / f\"{img_path.stem}.txt\"\n",
    "        if annotation_path.exists():\n",
    "            shutil.copy(annotation_path, dest_dir / 'labels' / annotation_path.name)\n",
    "\n",
    "# Copy files to the respective train and validation directories\n",
    "copy_files(train_files, train_dir)\n",
    "copy_files(val_files, val_dir)\n",
    "\n",
    "print(\"Dataset prepared and configuration saved to 'spacecraft.yaml'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.1.47 available ğŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.37 ğŸš€ Python-3.10.12 torch-2.2.2+cu121 CPU (AMD Ryzen 7 7730U with Radeon Graphics)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=spacecraft.yaml, epochs=3, time=None, patience=100, batch=64, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=finetune, name=train28, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=0.1, profile=False, freeze=20, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=finetune/train28\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLOv8n summary: 225 layers, 3011043 parameters, 3011027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.12.cv1.conv.weight'\n",
      "Freezing layer 'model.12.cv1.bn.weight'\n",
      "Freezing layer 'model.12.cv1.bn.bias'\n",
      "Freezing layer 'model.12.cv2.conv.weight'\n",
      "Freezing layer 'model.12.cv2.bn.weight'\n",
      "Freezing layer 'model.12.cv2.bn.bias'\n",
      "Freezing layer 'model.12.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.12.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.12.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.12.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.12.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.12.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.15.cv1.conv.weight'\n",
      "Freezing layer 'model.15.cv1.bn.weight'\n",
      "Freezing layer 'model.15.cv1.bn.bias'\n",
      "Freezing layer 'model.15.cv2.conv.weight'\n",
      "Freezing layer 'model.15.cv2.bn.weight'\n",
      "Freezing layer 'model.15.cv2.bn.bias'\n",
      "Freezing layer 'model.15.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.15.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.15.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.15.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.15.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.15.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.16.conv.weight'\n",
      "Freezing layer 'model.16.bn.weight'\n",
      "Freezing layer 'model.16.bn.bias'\n",
      "Freezing layer 'model.18.cv1.conv.weight'\n",
      "Freezing layer 'model.18.cv1.bn.weight'\n",
      "Freezing layer 'model.18.cv1.bn.bias'\n",
      "Freezing layer 'model.18.cv2.conv.weight'\n",
      "Freezing layer 'model.18.cv2.bn.weight'\n",
      "Freezing layer 'model.18.cv2.bn.bias'\n",
      "Freezing layer 'model.18.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.18.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.18.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.18.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.18.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.18.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.19.conv.weight'\n",
      "Freezing layer 'model.19.bn.weight'\n",
      "Freezing layer 'model.19.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /mnt/c/Python_work/spacecraft-pose-object-detection-runtime/data_dev/train/labels.cache... 2064 images, 0 backgrounds\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/c/Python_work/spacecraft-pose-object-detection-runtime/data_dev/val/labels.cache... 5161 images, 0 backgrounds, 0 \u001b[0m\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to finetune/train28/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mfinetune/train28\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/3         0G      1.461      3.397      1.301         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [10:42<00:00, 19.46s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [18:41<00:00, 27.36s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5161       5161      0.779      0.108      0.276      0.156\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/3         0G      1.508      2.706      1.366         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [09:29<00:00, 17.2\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 32/41 [14:10<"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_params = {\n",
    "    \"data\":'spacecraft.yaml',  # Path to your dataset configuration file\n",
    "    \"imgsz\":640,\n",
    "    \"epochs\":3,\n",
    "    \"fraction\":0.1,\n",
    "    \"batch\":64,\n",
    "    \"device\":'cpu',\n",
    "    \"project\":\"finetune\",\n",
    "    \"freeze\":20,\n",
    "    \"plots\":True\n",
    "}\n",
    "\n",
    "# Start the training process\n",
    "results = model.train(**train_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Initialize and load the model with weights in one line\n",
    "# model = YOLO('yolov8n.yaml').load('finetune/train20/weights/best.pt')\n",
    "\n",
    "# # Ensure the model is in evaluation mode if doing inference\n",
    "# model.eval()\n",
    "\n",
    "# # Move the model to the appropriate device\n",
    "# model.to('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mAP50 has increased to 0.438 with just 3 epoch of training.\n",
    "Training for longer period will definitely increase it upwards of 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "#import cv2\n",
    "def plot_image_with_bbox(image_path, results):\n",
    "    img = Image.open(image_path)\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "    for result in results:\n",
    "            for box in result.boxes:\n",
    "                left, top, right, bottom = np.array(box.xyxy.cpu(), dtype=np.int64).squeeze()\n",
    "                width = right - left\n",
    "                height = bottom - top\n",
    "                center = (left + int((right-left)/2), top + int((bottom-top)/2))\n",
    "                label = results[0].names[int(box.cls)]\n",
    "                confidence = float(box.conf.cpu())\n",
    "                #cv2.rectangle(img, (left, top),(right, bottom), (255, 0, 0), 2)\n",
    "                #cv2.putText(img, label,(left, bottom+20),cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                rect = patches.Rectangle((left, top), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "    plt.axis('off')  # Turn off axis\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = 'data_dev/train/images/000dbf763348037b46558bbcb6a032ac.png'\n",
    "# Run inference on with default arguments using finetuned model\n",
    "res_tuned = model.predict(filename, imgsz=640)\n",
    "plot_image_with_bbox(filename, res_tuned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with the pretrained model off the shelf\n",
    "orig = YOLO('yolov8n.yaml').load('yolov8n.pt')  # build from YAML and transfer weights\n",
    "res_tuned = orig.predict(filename, imgsz=640)\n",
    "plot_image_with_bbox(filename, res_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_DIRECTORY = Path.cwd()\n",
    "!cd {PROJ_DIRECTORY} && make pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker images | grep spacecraft-pose-object-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {PROJ_DIRECTORY} && make pack-benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {PROJ_DIRECTORY} && make test-submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We also provide a scoring script that computes your score using the same calculation that's used for the public leaderboard. You can generate a score for your local testing with a command like the one below. Remember that this score will be computed on your local test set, and your score on the public leaderboard will be based on an unseen test set.\n",
    "\n",
    "python scripts/score.py submission/submission.csv data/test_labels.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "To achieve better results one could try the following individuall or combine them\n",
    "- Train for longer time\n",
    "- Hyper paramter tuning(Unfreeze few more layers, drop out. etc)\n",
    "- Data augumentation\n",
    "- Other model (bigger model will help IOU but maynot satisfy realtime constraint; try first 3 before )\n",
    "\n",
    "Thanks for viewing this notebook. Hopefully it was helpful and please upvote if this notebook was useful or helped you to get started. Enjoy the competition ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4552256,
     "sourceId": 7879433,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
